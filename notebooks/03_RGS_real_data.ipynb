{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from RGS import FastRandomizedGreedySelection, RandomizedGreedySelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_splits(Xs, ys, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create train/test splits for multiple datasets stored in dictionaries.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Xs : dict\n",
    "        Dictionary containing feature matrices for each dataset\n",
    "    ys : dict\n",
    "        Dictionary containing target variables for each dataset\n",
    "    test_size : float, default=0.2\n",
    "        Proportion of the dataset to include in the test split\n",
    "    random_state : int, default=42\n",
    "        Random state for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing X_train, X_test, y_train, y_test for each dataset\n",
    "    \"\"\"\n",
    "    splits = {}\n",
    "    \n",
    "    for label in Xs.keys():\n",
    "        X = Xs[label]\n",
    "        y = ys[label]\n",
    "        \n",
    "        # Create train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            shuffle=True,\n",
    "            stratify=y if len(np.unique(y)) < 10 else None  # Stratify only for classification tasks\n",
    "        )\n",
    "        \n",
    "        # Store splits in dictionary\n",
    "        splits[label] = {\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test,\n",
    "            'train_size': len(X_train),\n",
    "            'test_size': len(X_test)\n",
    "        }\n",
    "    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "def train_regularized_models(splits, cv=5):\n",
    "    \"\"\"\n",
    "    Train and evaluate Lasso, Ridge, and ElasticNet models on multiple datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    splits : dict\n",
    "        Dictionary containing train/test splits for each dataset\n",
    "    cv : int, default=5\n",
    "        Number of cross-validation folds\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing trained models, predictions, and performance metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Define parameter grids for each model\n",
    "    param_grids = {\n",
    "        'Lasso': {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "        'Ridge': {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "        'ElasticNet': {\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "            'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Lasso': Lasso(random_state=42, max_iter=10000),\n",
    "        'Ridge': Ridge(random_state=42),\n",
    "        'ElasticNet': ElasticNet(random_state=42, max_iter=10000)\n",
    "    }\n",
    "    \n",
    "    for dataset_name in splits:\n",
    "        print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "        results[dataset_name] = {}\n",
    "        \n",
    "        # Get train/test data\n",
    "        X_train = splits[dataset_name]['X_train']\n",
    "        X_test = splits[dataset_name]['X_test']\n",
    "        y_train = splits[dataset_name]['y_train']\n",
    "        y_test = splits[dataset_name]['y_test']\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train and evaluate each model\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\nTraining {model_name}...\")\n",
    "            \n",
    "            # Perform grid search with cross-validation\n",
    "            grid_search = GridSearchCV(\n",
    "                model,\n",
    "                param_grids[model_name],\n",
    "                cv=cv,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            grid_search.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Get best model\n",
    "            best_model = grid_search.best_estimator_\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred_train = best_model.predict(X_train_scaled)\n",
    "            y_pred_test = best_model.predict(X_test_scaled)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "                'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "                'train_r2': r2_score(y_train, y_pred_train),\n",
    "                'test_r2': r2_score(y_test, y_pred_test),\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'cv_results': grid_search.cv_results_\n",
    "            }\n",
    "            \n",
    "            # Store results\n",
    "            results[dataset_name][model_name] = {\n",
    "                'model': best_model,\n",
    "                'predictions': {\n",
    "                    'train': y_pred_train,\n",
    "                    'test': y_pred_test\n",
    "                },\n",
    "                'metrics': metrics\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"Best parameters: {metrics['best_params']}\")\n",
    "            print(f\"Train RMSE: {metrics['train_rmse']:.4f}\")\n",
    "            print(f\"Test RMSE: {metrics['test_rmse']:.4f}\")\n",
    "            print(f\"Train R²: {metrics['train_r2']:.4f}\")\n",
    "            print(f\"Test R²: {metrics['test_r2']:.4f}\")\n",
    "            \n",
    "            # Print feature importance for Lasso and ElasticNet\n",
    "            if model_name in ['Lasso', 'ElasticNet']:\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'Feature': [f\"Feature_{i}\" for i in range(X_train.shape[1])],\n",
    "                    'Coefficient': best_model.coef_\n",
    "                })\n",
    "                feature_importance = feature_importance[feature_importance['Coefficient'] != 0]\n",
    "                print(\"\\nNon-zero coefficients:\")\n",
    "                print(feature_importance.sort_values(by='Coefficient', key=abs, ascending=False))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_rgs_m_parameter(X_train, y_train, X_test, y_test, k_max=15, cv=5):\n",
    "    \"\"\"\n",
    "    Tune the 'm' parameter for FastRandomizedGreedySelection using cross-validation.\n",
    "    Ensures m is always a Python int, not a numpy integer type.\n",
    "    \"\"\"\n",
    "    n_features = X_train.shape[1]\n",
    "    \n",
    "    # Generate values and convert to Python int\n",
    "    m_values = [int(m) for m in np.unique(np.linspace(1, n_features, 10, dtype=int))]\n",
    "    \n",
    "    # Verify types\n",
    "    assert all(isinstance(m, int) for m in m_values), \"All m values must be Python integers\"\n",
    "    \n",
    "    # Store results\n",
    "    cv_results = []\n",
    "    \n",
    "    print(\"Tuning 'm' parameter...\")\n",
    "    print(f\"Testing values: {m_values}\")\n",
    "    \n",
    "    # Evaluate each m value using cross-validation\n",
    "    for m in m_values:\n",
    "        assert isinstance(m, int), f\"m must be int, got {type(m)}\"\n",
    "        rgs = FastRandomizedGreedySelection(k_max=k_max, m=m)\n",
    "        scores = cross_val_score(rgs, X_train, y_train, cv=cv, scoring='r2')\n",
    "        mean_score = np.mean(scores)\n",
    "        std_score = np.std(scores)\n",
    "        \n",
    "        cv_results.append({\n",
    "            'm': m,\n",
    "            'mean_cv_score': mean_score,\n",
    "            'std_cv_score': std_score\n",
    "        })\n",
    "        \n",
    "        print(f\"m={m} (type={type(m)}): CV R² = {mean_score:.4f} (+/- {std_score:.4f})\")\n",
    "    \n",
    "    # Find best m value\n",
    "    best_result = max(cv_results, key=lambda x: x['mean_cv_score'])\n",
    "    best_m = best_result['m']  # Already a Python int from earlier conversion\n",
    "    \n",
    "    print(f\"\\nBest m value: {best_m} (type={type(best_m)})\")\n",
    "    print(f\"Best CV R²: {best_result['mean_cv_score']:.4f}\")\n",
    "    \n",
    "    # Train final model with best m value\n",
    "    best_model = FastRandomizedGreedySelection(k_max=k_max, m=best_m)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_score = best_model.score(X_test, y_test)\n",
    "    print(f\"\\nTest set R² with best m={best_m}: {test_score:.4f}\")\n",
    "    \n",
    "    # Selected features\n",
    "    selected_features = np.where(best_model.coef_ != 0)[0]\n",
    "    print(f\"\\nSelected features: {selected_features}\")\n",
    "    print(f\"Number of selected features: {len(selected_features)}\")\n",
    "    \n",
    "    return {\n",
    "        'cv_results': cv_results,\n",
    "        'best_m': best_m,\n",
    "        'best_cv_score': best_result['mean_cv_score'],\n",
    "        'test_score': test_score,\n",
    "        'best_model': best_model,\n",
    "        'selected_features': selected_features\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "def create_comparison_table(regularized_results, rgs_results, splits):\n",
    "    \"\"\"\n",
    "    Create a table comparing test MSE for all methods.\n",
    "    \"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    for dataset_name in splits.keys():\n",
    "        # Get predictions for regularized models\n",
    "        reg_models = regularized_results[dataset_name]\n",
    "        y_test = splits[dataset_name]['y_test']\n",
    "        \n",
    "        # Calculate MSE for each regularized model\n",
    "        for model_name in ['Lasso', 'Ridge', 'ElasticNet']:\n",
    "            y_pred = reg_models[model_name]['predictions']['test']\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Model': model_name,\n",
    "                'Test MSE': mse\n",
    "            })\n",
    "        \n",
    "        # Calculate MSE for RGS\n",
    "        rgs_model = rgs_results[dataset_name]['best_model']\n",
    "        X_test = splits[dataset_name]['X_test']\n",
    "        rgs_pred = rgs_model.predict(X_test)\n",
    "        rgs_mse = mean_squared_error(y_test, rgs_pred)\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Model': 'RGS',\n",
    "            'Test MSE': rgs_mse\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and format\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    df = df.pivot(index='Dataset', columns='Model', values='Test MSE')\n",
    "    \n",
    "    # Round values for readability\n",
    "    df = df.round(4)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Auto Pricing', 'Bodyfat']\n",
    "data = {}\n",
    "\n",
    "# Load data\n",
    "data['Auto Pricing'] = pd.read_csv('../real_data/207_autoPrice.tsv', sep='\\t')\n",
    "data['Satellite Image'] = pd.read_csv('../real_data/294_satellite_image.tsv', sep='\\t')\n",
    "data['Political'] = pd.read_csv('../real_data/201_pol.tsv', sep='\\t')\n",
    "data['Bodyfat'] = pd.read_csv('../real_data/560_bodyfat.tsv', sep='\\t')\n",
    "Xs = {label: data[label].drop('target', axis=1).values for label in labels}\n",
    "ys = {label: data[label]['target'].values for label in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the splits\n",
    "splits = create_train_test_splits(Xs, ys)\n",
    "\n",
    "# Print split information\n",
    "for label in splits:\n",
    "    print(f\"\\nDataset: {label}\")\n",
    "    print(f\"Training samples: {splits[label]['train_size']}\")\n",
    "    print(f\"Testing samples: {splits[label]['test_size']}\")\n",
    "    print(f\"X_train shape: {splits[label]['X_train'].shape}\")\n",
    "    print(f\"X_test shape: {splits[label]['X_test'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tuning for each dataset\n",
    "tuning_results = {}\n",
    "for dataset_name in splits:\n",
    "    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "    X_train = splits[dataset_name]['X_train']\n",
    "    X_test = splits[dataset_name]['X_test']\n",
    "    y_train = splits[dataset_name]['y_train']\n",
    "    y_test = splits[dataset_name]['y_test']\n",
    "    n_features = X_train.shape[1]\n",
    "    tuning_results[dataset_name] = tune_rgs_m_parameter(X_train, y_train, X_test, y_test, k_max=n_features//3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "results = train_regularized_models(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012463106871794572\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rgs1</th>\n",
       "      <th>rgs2</th>\n",
       "      <th>rgs3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>457.946755</td>\n",
       "      <td>457.946755</td>\n",
       "      <td>457.946755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.958522</td>\n",
       "      <td>1.128858</td>\n",
       "      <td>1.191185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.292462</td>\n",
       "      <td>0.272517</td>\n",
       "      <td>0.406167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125505</td>\n",
       "      <td>0.151693</td>\n",
       "      <td>0.186742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.072272</td>\n",
       "      <td>0.064924</td>\n",
       "      <td>0.077382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.041882</td>\n",
       "      <td>0.041343</td>\n",
       "      <td>0.047635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.024652</td>\n",
       "      <td>0.022743</td>\n",
       "      <td>0.031133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.016293</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>0.021527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.010693</td>\n",
       "      <td>0.009187</td>\n",
       "      <td>0.011942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007404</td>\n",
       "      <td>0.007755</td>\n",
       "      <td>0.008140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.006517</td>\n",
       "      <td>0.006388</td>\n",
       "      <td>0.006873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.006485</td>\n",
       "      <td>0.006453</td>\n",
       "      <td>0.006425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.007106</td>\n",
       "      <td>0.006962</td>\n",
       "      <td>0.007014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.008028</td>\n",
       "      <td>0.008040</td>\n",
       "      <td>0.008174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.009284</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>0.009582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.010807</td>\n",
       "      <td>0.010718</td>\n",
       "      <td>0.010942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rgs1        rgs2        rgs3\n",
       "0   457.946755  457.946755  457.946755\n",
       "1     0.958522    1.128858    1.191185\n",
       "2     0.292462    0.272517    0.406167\n",
       "3     0.125505    0.151693    0.186742\n",
       "4     0.072272    0.064924    0.077382\n",
       "5     0.041882    0.041343    0.047635\n",
       "6     0.024652    0.022743    0.031133\n",
       "7     0.016293    0.013587    0.021527\n",
       "8     0.010693    0.009187    0.011942\n",
       "9     0.007404    0.007755    0.008140\n",
       "10    0.006517    0.006388    0.006873\n",
       "11    0.006485    0.006453    0.006425\n",
       "12    0.007106    0.006962    0.007014\n",
       "13    0.008028    0.008040    0.008174\n",
       "14    0.009284    0.009242    0.009582\n",
       "15    0.010807    0.010718    0.010942"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create comparison table\n",
    "comparison_table = create_comparison_table(results, tuning_results, splits)\n",
    "\n",
    "# Display table\n",
    "print(\"\\nTest MSE Comparison:\")\n",
    "print(comparison_table)\n",
    "\n",
    "# Find best model for each dataset\n",
    "best_models = comparison_table.idxmin(axis=1)\n",
    "print(\"\\nBest performing model for each dataset:\")\n",
    "print(best_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
