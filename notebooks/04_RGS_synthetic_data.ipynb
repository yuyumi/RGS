{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from RGS import FastRandomizedGreedySelection, RandomizedGreedySelection\n",
    "from data_plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_datasets(data):\n",
    "    \"\"\"\n",
    "    Clean datasets by removing missing values and duplicate rows.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : dict\n",
    "        Dictionary containing pandas DataFrames for each dataset\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing cleaned DataFrames\n",
    "    \"\"\"\n",
    "    cleaned_data = {}\n",
    "    \n",
    "    for label, df in data.items():\n",
    "        print(f\"\\nCleaning dataset: {label}\")\n",
    "        print(f\"Original shape: {df.shape}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_before = df.isnull().sum().sum()\n",
    "        print(f\"Missing values before: {missing_before}\")\n",
    "        \n",
    "        # Check for duplicates\n",
    "        duplicates_before = df.duplicated().sum()\n",
    "        print(f\"Duplicate rows before: {duplicates_before}\")\n",
    "        \n",
    "        # Remove missing values\n",
    "        df_cleaned = df.dropna()\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df_cleaned = df_cleaned.drop_duplicates()\n",
    "        \n",
    "        # Final checks\n",
    "        missing_after = df_cleaned.isnull().sum().sum()\n",
    "        duplicates_after = df_cleaned.duplicated().sum()\n",
    "        \n",
    "        print(f\"Final shape: {df_cleaned.shape}\")\n",
    "        print(f\"Rows removed due to missing values: {len(df) - len(df_cleaned)}\")\n",
    "        print(f\"Missing values after: {missing_after}\")\n",
    "        print(f\"Duplicate rows after: {duplicates_after}\")\n",
    "        \n",
    "        cleaned_data[label] = df_cleaned\n",
    "    \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_synthetic_data(df, n_row_duplicates=2, n_col_duplicates=2, random_seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic data efficiently by repeating base rows and adding permuted columns.\n",
    "    Base columns remain identical across row duplications.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame\n",
    "    n_row_duplicates : int, default=2\n",
    "        Number of times to repeat the original rows\n",
    "    n_col_duplicates : int, default=2\n",
    "        Number of permuted column sets to create\n",
    "    random_seed : int, optional\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with repeated base rows and permuted additional columns\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    # Calculate total number of rows in advance\n",
    "    n_original_rows = len(df)\n",
    "    total_rows = n_original_rows * (n_row_duplicates + 1)\n",
    "    \n",
    "    # Pre-allocate the final dictionary with numpy arrays\n",
    "    data_dict = {}\n",
    "    \n",
    "    # Efficiently handle base columns using numpy repeat\n",
    "    for col in df.columns:\n",
    "        data_dict[col] = np.repeat(df[col].values, n_row_duplicates + 1)\n",
    "    \n",
    "    # Generate all permutations at once for each original column\n",
    "    rng = np.random.default_rng(random_seed)  # Use newer random generator\n",
    "    \n",
    "    # Pre-generate all permutation indices\n",
    "    all_perm_indices = np.array([\n",
    "        rng.permutation(total_rows) \n",
    "        for _ in range(n_col_duplicates * len(df.columns))\n",
    "    ])\n",
    "    \n",
    "    # Efficiently generate permuted columns using broadcasting\n",
    "    perm_idx = 0\n",
    "    repeated_values = {\n",
    "        col: np.tile(df[col].values, n_row_duplicates + 1)\n",
    "        for col in df.columns\n",
    "    }\n",
    "    \n",
    "    for i in range(n_col_duplicates):\n",
    "        for col in df.columns:\n",
    "            new_col = f\"{col}_perm_{i+1}\"\n",
    "            data_dict[new_col] = repeated_values[col][all_perm_indices[perm_idx]]\n",
    "            perm_idx += 1\n",
    "    \n",
    "    return pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "from RGS import FastRandomizedGreedySelection\n",
    "\n",
    "def run_synthetic_experiments(data_dict, n_iter=5, test_size=0.2, n_row_duplicates=2, \n",
    "                            n_col_duplicates=2, random_seed=None):\n",
    "    \"\"\"\n",
    "    Run experiments with synthetic data, tracking performance across iterations.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Set up base parameter grids for standard models\n",
    "    base_param_grids = {\n",
    "        'Lasso': {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "        'Ridge': {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "        'ElasticNet': {\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "            'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Initialize base models\n",
    "    models = {\n",
    "        'Lasso': Lasso(random_state=42, max_iter=10000),\n",
    "        'Ridge': Ridge(random_state=42),\n",
    "        'ElasticNet': ElasticNet(random_state=42, max_iter=10000)\n",
    "    }\n",
    "    \n",
    "    # Main iteration loop with progress bar\n",
    "    with tqdm(total=n_iter * len(data_dict), desc=\"Running experiments\") as pbar:\n",
    "        for iteration in range(n_iter):\n",
    "            iter_seed = random_seed + iteration if random_seed else None\n",
    "            \n",
    "            # Process each dataset\n",
    "            for dataset_name, original_df in data_dict.items():\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Generate synthetic data\n",
    "                synthetic_df = generate_synthetic_data(\n",
    "                    original_df, \n",
    "                    n_row_duplicates=n_row_duplicates,\n",
    "                    n_col_duplicates=n_col_duplicates,\n",
    "                    random_seed=iter_seed\n",
    "                )\n",
    "                \n",
    "                # Split features and target\n",
    "                X = synthetic_df.drop('target', axis=1) \n",
    "                y = synthetic_df['target']              \n",
    "                \n",
    "                n_features = X.shape[1]\n",
    "                \n",
    "                # Create dynamic parameter grid for FastRGS\n",
    "                m_values = [int(m) for m in np.unique(np.linspace(1, n_features, 10, dtype=int))]\n",
    "                # Calculate max k_max based on feature count divided by duplicates\n",
    "                max_k = int(n_features / (n_col_duplicates + 1))  # +1 because we have original columns too\n",
    "                k_max_values = [int(k) for k in np.unique(np.linspace(2, max_k, 10, dtype=int))]\n",
    "                k_max_values = [k for k in k_max_values if k >= 2]  # Ensure k_max is at least 2\n",
    "                \n",
    "                # Initialize model and parameter grid dictionaries\n",
    "                current_models = models.copy()\n",
    "                current_param_grids = base_param_grids.copy()\n",
    "\n",
    "                # Create FastRGS models for each (k_max, m) combination\n",
    "                for k_max in k_max_values:\n",
    "                    for m in m_values:\n",
    "                        if m >= k_max:  # m must be >= k_max\n",
    "                            model_name = f'FastRGS_k{k_max}_m{m}'\n",
    "                            current_models[model_name] = FastRandomizedGreedySelection(\n",
    "                                k_max=k_max,\n",
    "                                m=m,\n",
    "                                n_resample_iter=7\n",
    "                            )\n",
    "                            current_param_grids[model_name] = {}\n",
    "                \n",
    "                # Train-test split\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=test_size, random_state=iter_seed\n",
    "                )\n",
    "                \n",
    "                # Scale features\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                \n",
    "                # Train and evaluate each model\n",
    "                for model_name, model in current_models.items():\n",
    "                    try:\n",
    "                        # Grid search\n",
    "                        grid_search = GridSearchCV(\n",
    "                            model,\n",
    "                            current_param_grids[model_name],\n",
    "                            cv=5,\n",
    "                            scoring='r2',\n",
    "                            n_jobs=-1,\n",
    "                            verbose=0\n",
    "                        )\n",
    "                        \n",
    "                        grid_search.fit(X_train_scaled, y_train)\n",
    "                        best_model = grid_search.best_estimator_\n",
    "                        \n",
    "                        # Get predictions\n",
    "                        y_pred_train = best_model.predict(X_train_scaled)\n",
    "                        y_pred_test = best_model.predict(X_test_scaled)\n",
    "                        \n",
    "                        # Calculate metrics\n",
    "                        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "                        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "                        train_r2 = r2_score(y_train, y_pred_train)\n",
    "                        test_r2 = r2_score(y_test, y_pred_test)\n",
    "                        \n",
    "                        # Get selected features for FastRGS\n",
    "                        n_selected_features = None\n",
    "                        if model_name == 'FastRGS':\n",
    "                            n_selected_features = np.sum(best_model.coef_ != 0)\n",
    "                        \n",
    "                        # Store results\n",
    "                        results.append({\n",
    "                            'iteration': iteration,\n",
    "                            'dataset': dataset_name,\n",
    "                            'model': model_name,\n",
    "                            'train_rmse': train_rmse,\n",
    "                            'test_rmse': test_rmse,\n",
    "                            'train_r2': train_r2,\n",
    "                            'test_r2': test_r2,\n",
    "                            'best_params': str(grid_search.best_params_),\n",
    "                            'n_features': n_features,\n",
    "                            'n_samples': len(X),\n",
    "                            'n_selected_features': n_selected_features,\n",
    "                            'processing_time': time.time() - start_time\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with {model_name} on {dataset_name}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'synthetic_experiments_results_{timestamp}.csv'\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_comparison_tables(results_df):\n",
    "    \"\"\"\n",
    "    Create comparison tables from simulation results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : pandas.DataFrame\n",
    "        DataFrame containing simulation results\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing different comparison tables\n",
    "    \"\"\"\n",
    "    tables = {}\n",
    "    \n",
    "    # 1. Average Performance Table (across all iterations)\n",
    "    perf_metrics = ['test_rmse', 'test_r2', 'train_rmse', 'train_r2']\n",
    "    avg_performance = results_df.groupby(['dataset', 'model'])[perf_metrics].agg(\n",
    "        ['mean', 'std']\n",
    "    ).round(4)\n",
    "    \n",
    "    # Reshape for better readability\n",
    "    avg_performance_flat = pd.DataFrame()\n",
    "    for metric in perf_metrics:\n",
    "        mean_col = f\"{metric}_mean\"\n",
    "        std_col = f\"{metric}_std\"\n",
    "        avg_performance_flat[mean_col] = avg_performance[metric]['mean']\n",
    "        avg_performance_flat[std_col] = avg_performance[metric]['std']\n",
    "    \n",
    "    tables['average_performance'] = avg_performance_flat.reset_index()\n",
    "    \n",
    "    # 2. Best Model Per Dataset\n",
    "    best_models = (results_df.groupby(['dataset', 'model'])['test_r2']\n",
    "                  .mean()\n",
    "                  .reset_index()\n",
    "                  .sort_values('test_r2', ascending=False)\n",
    "                  .groupby('dataset').first()\n",
    "                  .reset_index())\n",
    "    tables['best_models'] = best_models\n",
    "    \n",
    "    # 3. Feature Selection Summary (for FastRGS)\n",
    "    if 'n_selected_features' in results_df.columns:\n",
    "        feature_selection = (results_df[results_df['model'] == 'FastRGS']\n",
    "                           .groupby('dataset')\n",
    "                           .agg({\n",
    "                               'n_selected_features': ['mean', 'std'],\n",
    "                               'n_features': 'first'\n",
    "                           })\n",
    "                           .round(2))\n",
    "        feature_selection.columns = ['avg_selected', 'std_selected', 'total_features']\n",
    "        tables['feature_selection'] = feature_selection.reset_index()\n",
    "    \n",
    "    # 4. Processing Time Comparison\n",
    "    time_comparison = (results_df.groupby(['dataset', 'model'])['processing_time']\n",
    "                      .agg(['mean', 'std'])\n",
    "                      .round(2)\n",
    "                      .reset_index())\n",
    "    tables['processing_time'] = time_comparison\n",
    "    \n",
    "    # 5. Model Rankings\n",
    "    rankings = (results_df.groupby(['dataset', 'model'])['test_r2']\n",
    "               .mean()\n",
    "               .reset_index()\n",
    "               .pivot(index='dataset', columns='model', values='test_r2')\n",
    "               .rank(axis=1, ascending=False)\n",
    "               .round(2))\n",
    "    tables['model_rankings'] = rankings.reset_index()\n",
    "    \n",
    "    # 6. Best Parameters Summary\n",
    "    best_params = (results_df.groupby(['dataset', 'model'])\n",
    "                  .agg({'best_params': lambda x: x.mode().iloc[0] if len(x) > 0 else None})\n",
    "                  .reset_index())\n",
    "    tables['best_parameters'] = best_params\n",
    "    \n",
    "    # Save all tables to Excel with multiple sheets\n",
    "    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "    excel_filename = f'comparison_tables_{timestamp}.xlsx'\n",
    "    \n",
    "    with pd.ExcelWriter(excel_filename) as writer:\n",
    "        for table_name, table in tables.items():\n",
    "            table.to_excel(writer, sheet_name=table_name, index=False)\n",
    "            \n",
    "    return tables\n",
    "\n",
    "# Example usage:\n",
    "# tables = create_comparison_tables(results_df)\n",
    "# \n",
    "# # Access individual tables\n",
    "# print(\"Average Performance:\")\n",
    "# print(tables['average_performance'])\n",
    "# \n",
    "# print(\"\\nBest Models per Dataset:\")\n",
    "# print(tables['best_models'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning dataset: Auto Pricing\n",
      "Original shape: (159, 16)\n",
      "Missing values before: 0\n",
      "Duplicate rows before: 0\n",
      "Final shape: (159, 16)\n",
      "Rows removed due to missing values: 0\n",
      "Missing values after: 0\n",
      "Duplicate rows after: 0\n",
      "\n",
      "Cleaning dataset: Sunspots\n",
      "Original shape: (235, 13)\n",
      "Missing values before: 0\n",
      "Duplicate rows before: 0\n",
      "Final shape: (235, 13)\n",
      "Rows removed due to missing values: 0\n",
      "Missing values after: 0\n",
      "Duplicate rows after: 0\n",
      "\n",
      "Cleaning dataset: Bodyfat\n",
      "Original shape: (252, 15)\n",
      "Missing values before: 0\n",
      "Duplicate rows before: 0\n",
      "Final shape: (252, 15)\n",
      "Rows removed due to missing values: 0\n",
      "Missing values after: 0\n",
      "Duplicate rows after: 0\n",
      "\n",
      "Cleaning dataset: PW\n",
      "Original shape: (200, 11)\n",
      "Missing values before: 0\n",
      "Duplicate rows before: 0\n",
      "Final shape: (200, 11)\n",
      "Rows removed due to missing values: 0\n",
      "Missing values after: 0\n",
      "Duplicate rows after: 0\n"
     ]
    }
   ],
   "source": [
    "labels = ['Auto Pricing', 'Bodyfat', 'Sunspots', 'PW']\n",
    "data = {}\n",
    "\n",
    "# Load data\n",
    "data['Auto Pricing'] = pd.read_csv('../real_data/207_autoPrice.tsv', sep='\\t')\n",
    "data['Sunspots'] = pd.read_csv('../real_data/695_chatfield_4.tsv', sep='\\t')\n",
    "data['Bodyfat'] = pd.read_csv('../real_data/560_bodyfat.tsv', sep='\\t')\n",
    "# data['Pharynx'] = pd.read_csv('../real_data/1196_BNG_pharynx.tsv', sep='\\t')\n",
    "data['PW'] = pd.read_csv('../real_data/229_pwLinear.tsv', sep='\\t')\n",
    "# data['CPU'] = pd.read_csv('../real_data/197_cpu_act.tsv', sep='\\t')\n",
    "# data['House'] = pd.read_csv('../real_data/574_house_16H.tsv', sep='\\t')\n",
    "# data['MeatFat'] = pd.read_csv('../real_data/505_tecator.tsv', sep='\\t')\n",
    "\n",
    "# Clean the datasets\n",
    "cleaned_data = clean_datasets(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39308d9098dd493582bc6586387c73bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running experiments:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = run_synthetic_experiments(\n",
    "    data_dict=cleaned_data,\n",
    "    n_iter=5,  # Number of iterations\n",
    "    n_row_duplicates=0,  # Number of times to repeat rows\n",
    "    n_col_duplicates=15,  # Number of permuted column sets\n",
    "    random_seed=42  # For reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = create_comparison_tables(results)\n",
    "\n",
    "# Access individual tables\n",
    "# print(\"Average Performance:\")\n",
    "# print(tables['average_performance'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Models per Dataset:\n",
      "        dataset            model   test_r2\n",
      "0  Auto Pricing   FastRGS_k2_m57  0.748633\n",
      "1       Bodyfat  FastRGS_k2_m212  0.987525\n",
      "2            PW  FastRGS_k6_m175  0.736023\n",
      "3      Sunspots   FastRGS_k2_m69  0.886556\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBest Models per Dataset:\")\n",
    "print(tables['best_models'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
