{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List\n",
    "import time\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from RGS import FastRandomizedGreedySelection, RandomizedGreedySelection\n",
    "from data_generator import *\n",
    "from data_plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_df(y, y_true, y_pred, n, sigma):\n",
    "    \"\"\"Calculate degrees of freedom using the provided formula.\"\"\"\n",
    "    train_error = np.mean((y - y_pred)**2)\n",
    "    insample_error = np.mean((y_true - y_pred)**2)\n",
    "    df = n/(2*sigma**2) * (insample_error - train_error + sigma**2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_penalty(m: int, omega: List[float], sigma2: float, n: int) -> float:\n",
    "    \"\"\"Compute the penalty term for feature selection using sorted coefficients.\"\"\"\n",
    "    omega_sorted = sorted(omega, reverse=True)\n",
    "    return (2 * sigma2**2 / n) * sum(np.log(p / (j+1)) * omega_sorted[j] for j in range(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_penalized_score(model, X, y_true, k, sigma2, n, p):\n",
    "    \"\"\"\n",
    "    Compute penalized score using MSE + penalty.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : RGS/GS model\n",
    "        Fitted model with .coef_ attribute\n",
    "    X : ndarray\n",
    "        Input design matrix\n",
    "    y_true : ndarray\n",
    "        True signal (without noise)\n",
    "    k : int\n",
    "        Number of features to use\n",
    "    sigma2 : float\n",
    "        True noise variance\n",
    "    n : int\n",
    "        Sample size\n",
    "    p : int\n",
    "        Number of features\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Penalized score (MSE + penalty)\n",
    "    \"\"\"\n",
    "    # Get predictions and MSE\n",
    "    y_pred = model.predict(X, k=k)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "#     # Get coefficients and sort them by absolute value\n",
    "#     coef = np.abs(model.coef_)  # Get absolute values\n",
    "#     coef_sorted = np.sort(coef)[::-1]  # Sort in descending order\n",
    "    \n",
    "#     # Use top k coefficients\n",
    "#     coef_k = coef_sorted[:k]\n",
    "    \n",
    "    # Compute penalty\n",
    "    penalty = 2*sigma2**2/n*k*np.log(p/k)\n",
    "    \n",
    "    return mse + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(n_predictors=250, n_train=2000, signal_proportion=0.04, cov='orthogonal', n_sim=10):\n",
    "    \"\"\"\n",
    "    Run simulation comparing different methods, varying k_max during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate base design matrix\n",
    "    X_generators = {\n",
    "        'orthogonal': generate_orthogonal_X,\n",
    "        'banded': generate_banded_X\n",
    "    }\n",
    "    if isinstance(cov, str):\n",
    "        if cov not in X_generators:\n",
    "            raise ValueError(f\"Unknown generator: {cov}. Available generators: {list(X_generators.keys())}\")\n",
    "        X_generator = X_generators[cov]\n",
    "    else:\n",
    "        # Assume it's a callable\n",
    "        X_generator = cov\n",
    "    X = X_generator(n_predictors, n_train)\n",
    "    \n",
    "    # Define k values for training and m values for RGS\n",
    "    k_values = list(range(5, 31, 5))  # [5, 10, 15, ..., 50]\n",
    "    base = 2\n",
    "    num_points = 7\n",
    "    m_values = [int(2 + (n_predictors-2) * (base**x - 1)/(base**(num_points-1) - 1)) \n",
    "            for x in range(num_points)]\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "    \n",
    "    # Define example generators with different noise levels\n",
    "    sigmas = [0.01, 0.5, 1, 3, 5, 7]\n",
    "#     sigmas = [10, 20, 30, 35, 40, 45]\n",
    "    example_generators = {\n",
    "#         f'sigma_{sigmas[0]}': generate_exact_sparsity_example,\n",
    "#         f'sigma_{sigmas[1]}': generate_exact_sparsity_example,\n",
    "#         f'sigma_{sigmas[2]}': generate_exact_sparsity_example,\n",
    "#         f'sigma_{sigmas[3]}': generate_exact_sparsity_example,\n",
    "#         f'sigma_{sigmas[4]}': generate_exact_sparsity_example,\n",
    "#         f'sigma_{sigmas[5]}': generate_exact_sparsity_example\n",
    "#         f'sigma_{sigmas[0]}_inexact': generate_inexact_sparsity_example,\n",
    "#         f'sigma_{sigmas[1]}_inexact': generate_inexact_sparsity_example,\n",
    "#         f'sigma_{sigmas[2]}_inexact': generate_inexact_sparsity_example,\n",
    "#         f'sigma_{sigmas[3]}_inexact': generate_inexact_sparsity_example,\n",
    "#         f'sigma_{sigmas[4]}_inexact': generate_inexact_sparsity_example,\n",
    "#         f'sigma_{sigmas[5]}_inexact': generate_inexact_sparsity_example\n",
    "#         'sigma_10_linear': generate_nonlinear_example,\n",
    "#         'sigma_10_highly_nonlinear': generate_nonlinear_example,\n",
    "#         'sigma_60_small_snr': generate_small_snr,\n",
    "#         f'sigma_{sigmas[0]}_laplace': generate_laplace_example,\n",
    "#         f'sigma_{sigmas[1]}_laplace': generate_laplace_example,\n",
    "#         f'sigma_{sigmas[2]}_laplace': generate_laplace_example\n",
    "#         f'sigma_{sigmas[0]}_cauchy': generate_cauchy_example,\n",
    "#         f'sigma_{sigmas[1]}_cauchy': generate_cauchy_example,\n",
    "#         f'sigma_{sigmas[2]}_cauchy': generate_cauchy_example\n",
    "    }\n",
    "    \n",
    "    # Create progress bar for total iterations\n",
    "    total_iterations = n_sim * len(example_generators)\n",
    "    pbar = tqdm(total=total_iterations, desc=\"Overall Progress\")\n",
    "    \n",
    "    # Time the first iteration separately to get a good estimate\n",
    "    first_iter_time = None\n",
    "    \n",
    "    for sim in range(n_sim):\n",
    "        i = 0\n",
    "        for noise_level, generator in example_generators.items():\n",
    "            iter_start_time = time.time()\n",
    "            # Generate data for this simulation\n",
    "#             if (noise_level == 'sigma_10_linear'):\n",
    "#                 flag = 0\n",
    "#             else:\n",
    "#                 flag = 1\n",
    "#             X, y, y_true, p, sigma = generator(X, seed=sim, eta=flag)\n",
    "            X, y, y_true, beta_true, p, sigma = generator(X, signal_proportion, sigmas[i], seed=sim)\n",
    "            \n",
    "            result = {\n",
    "                'simulation': sim,\n",
    "                'noise_level': noise_level,\n",
    "                'sigma': sigma\n",
    "            }\n",
    "            \n",
    "            # Fit Lasso\n",
    "            lasso = LassoCV(cv=10, random_state=sim)\n",
    "            lasso.fit(X, y)\n",
    "            y_pred_lasso = lasso.predict(X)\n",
    "            result['mse_lasso'] = mean_squared_error(y_true, y_pred_lasso)\n",
    "            result['df_lasso'] = calculate_df(y, y_true, y_pred_lasso, n_train, sigma)\n",
    "            result['coef_recovery_lasso'] = np.mean((lasso.coef_ - beta_true)**2)\n",
    "            result['support_recovery_lasso'] = np.mean((lasso.coef_ != 0) == (beta_true != 0))\n",
    "            \n",
    "            # Fit Ridge\n",
    "            ridge = RidgeCV(cv=10)\n",
    "            ridge.fit(X, y)\n",
    "            y_pred_ridge = ridge.predict(X)\n",
    "            result['mse_ridge'] = mean_squared_error(y_true, y_pred_ridge)\n",
    "            result['df_ridge'] = calculate_df(y, y_true, y_pred_ridge, n_train, sigma)\n",
    "            result['coef_recovery_ridge'] = np.mean((ridge.coef_ - beta_true)**2)\n",
    "            \n",
    "            # Fit Elastic Net\n",
    "            elastic = ElasticNetCV(cv=10, random_state=sim)\n",
    "            elastic.fit(X, y)\n",
    "            y_pred_elastic = elastic.predict(X)\n",
    "            result['mse_elastic'] = mean_squared_error(y_true, y_pred_elastic)\n",
    "            result['df_elastic'] = calculate_df(y, y_true, y_pred_elastic, n_train, sigma)\n",
    "            result['coef_recovery_elastic'] = np.mean((elastic.coef_ - beta_true)**2)\n",
    "            result['support_recovery_elastic'] = np.mean((elastic.coef_ != 0) == (beta_true != 0))\n",
    "            \n",
    "            # Fit FGS\n",
    "            mse_fgs = {}\n",
    "            for k_max in k_values:\n",
    "                fgs = FastRandomizedGreedySelection(k_max=k_max, m=n_predictors, n_resample_iter=7)\n",
    "                fgs.fit(X, y)\n",
    "                y_pred_fgs = fgs.predict(X, k=k_max)\n",
    "                result[f'mse_gs_k{k_max}'] = mean_squared_error(y_true, y_pred_fgs)\n",
    "                result[f'df_gs_k{k_max}'] = calculate_df(y, y_true, y_pred_fgs, n_train, sigma)\n",
    "                result[f'pen_gs_k{k_max}'] = compute_penalized_score(fgs, X, y_true, k_max, sigma, n_train, p)\n",
    "                result[f'coef_recovery_gs_k{k_max}'] = np.mean((fgs.coef_ - beta_true)**2)\n",
    "                result[f'support_recovery_gs_k{k_max}'] = np.mean((fgs.coef_ != 0) == (beta_true != 0))\n",
    "            \n",
    "            # Fit RGS\n",
    "            mse_rgs = {}\n",
    "            for m in m_values:\n",
    "                for k_max in k_values:\n",
    "                    rgs = FastRandomizedGreedySelection(k_max=k_max, m=m, n_resample_iter=7)\n",
    "                    rgs.fit(X, y)\n",
    "                    y_pred_rgs = rgs.predict(X, k=k_max)\n",
    "                    result[f'mse_rgs_m{m}_k{k_max}'] = mean_squared_error(y_true, y_pred_rgs)\n",
    "                    result[f'df_rgs_m{m}_k{k_max}'] = calculate_df(y, y_true, y_pred_rgs, n_train, sigma)\n",
    "                    result[f'pen_rgs_m{m}_k{k_max}'] = compute_penalized_score(rgs, X, y_true, k_max, sigma, n_train, p)\n",
    "                    result[f'coef_recovery_rgs_m{m}_k{k_max}'] = np.mean((rgs.coef_ - beta_true)**2)\n",
    "                    result[f'support_recovery_rgs_m{m}_k{k_max}'] = np.mean((rgs.coef_ != 0) == (beta_true != 0))\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Calculate timing information\n",
    "            iter_time = time.time() - iter_start_time\n",
    "            if first_iter_time is None:\n",
    "                first_iter_time = iter_time\n",
    "                estimated_total_time = first_iter_time * total_iterations\n",
    "            \n",
    "            iterations_completed = sim * len(example_generators) + list(example_generators.keys()).index(noise_level) + 1\n",
    "            time_elapsed = time.time() - start_time\n",
    "            time_per_iter = time_elapsed / iterations_completed\n",
    "            estimated_remaining_time = time_per_iter * (total_iterations - iterations_completed)\n",
    "            \n",
    "            # Update progress bar with timing information\n",
    "            pbar.set_postfix({\n",
    "                'Simulation': f'{sim + 1}/{n_sim}',\n",
    "                'Noise': noise_level,\n",
    "                'Iter Time': f'{iter_time:.1f}s',\n",
    "                'Est. Remaining': f'{estimated_remaining_time/60:.1f}min',\n",
    "                'Est. Total': f'{(time_elapsed + estimated_remaining_time)/60:.1f}min'\n",
    "            })\n",
    "            pbar.update(1)\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "    # Close progress bar\n",
    "    pbar.close()\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nSimulation completed in {total_time/60:.1f} minutes\")\n",
    "    print(f\"Average time per iteration: {total_time/total_iterations:.1f} seconds\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate summary statistics for both MSE and DF\n",
    "    metrics = ['mse', 'df']\n",
    "    base_methods = ['lasso', 'ridge', 'elastic']\n",
    "    agg_cols = {}\n",
    "    \n",
    "    # Add base methods\n",
    "    for metric in metrics:\n",
    "        for method in base_methods:\n",
    "            col = f'{metric}_{method}'\n",
    "            agg_cols[col] = ['mean', 'std']\n",
    "    \n",
    "    # Add FGS methods\n",
    "    for metric in metrics:\n",
    "        for k in k_values:\n",
    "            col = f'{metric}_gs_k{k}'\n",
    "            agg_cols[col] = ['mean', 'std']\n",
    "    \n",
    "    # Add RGS methods\n",
    "    for metric in metrics:\n",
    "        for m in m_values:\n",
    "            for k in k_values:\n",
    "                col = f'{metric}_rgs_m{m}_k{k}'\n",
    "                agg_cols[col] = ['mean', 'std']\n",
    "    \n",
    "    summary = results_df.groupby('noise_level').agg(agg_cols).round(4)\n",
    "    \n",
    "    # Save results with timestamp\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_df.to_csv(f'../data/simulation_results_{timestamp}.csv', index=False)\n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    summary.to_csv(f'../data/simulation_summary_{timestamp}.csv')\n",
    "    \n",
    "    print(f\"\\nResults saved to simulation_results_{timestamp}.csv\")\n",
    "    print(f\"Summary saved to simulation_summary_{timestamp}.csv\")\n",
    "    \n",
    "    return results_df, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_optimal_parameters(results_df):\n",
    "    \"\"\"\n",
    "    Analyze the results to find optimal k and m values for each noise level.\n",
    "    Also includes degrees of freedom analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : DataFrame\n",
    "        Results from run_simulation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with optimal parameters and corresponding MSE and DF values\n",
    "    \"\"\"\n",
    "    noise_levels = sorted(results_df['noise_level'].unique())\n",
    "    optimal_params = []\n",
    "    \n",
    "    for noise in noise_levels:\n",
    "        noise_data = results_df[results_df['noise_level'] == noise]\n",
    "        \n",
    "        # Find best GS k value based on penalized\n",
    "        gs_mse_cols = [col for col in noise_data.columns if col.startswith('pen_gs_k')]\n",
    "        best_gs_col = min(gs_mse_cols, key=lambda x: noise_data[x].mean())\n",
    "        best_gs_k = int(best_gs_col.replace('pen_gs_k', ''))\n",
    "        best_gs_mse = noise_data[f'mse_gs_k{best_gs_k}'].mean()\n",
    "        best_gs_df = noise_data[f'df_gs_k{best_gs_k}'].mean()\n",
    "        \n",
    "        # Find best RGS m and k values based on penalized\n",
    "        rgs_mse_cols = [col for col in noise_data.columns if col.startswith('pen_rgs_m')]\n",
    "        best_rgs_col = min(rgs_mse_cols, key=lambda x: noise_data[x].mean())\n",
    "        # Extract m and k values using string replacement\n",
    "        col_parts = best_rgs_col.replace('pen_rgs_m', '').split('_k')\n",
    "        m_val = int(col_parts[0])\n",
    "        k_val = int(col_parts[1])\n",
    "        best_rgs_mse = noise_data[f'mse_rgs_m{m_val}_k{k_val}'].mean()\n",
    "        best_rgs_df = noise_data[f'df_rgs_m{m_val}_k{k_val}'].mean()\n",
    "        \n",
    "        # Get baseline methods performance\n",
    "        baseline_metrics = {\n",
    "            'lasso_mse': noise_data['mse_lasso'].mean(),\n",
    "            'lasso_df': noise_data['df_lasso'].mean(),\n",
    "            'ridge_mse': noise_data['mse_ridge'].mean(),\n",
    "            'ridge_df': noise_data['df_ridge'].mean(),\n",
    "            'elastic_mse': noise_data['mse_elastic'].mean(),\n",
    "            'elastic_df': noise_data['df_elastic'].mean()\n",
    "        }\n",
    "        \n",
    "        # Store results\n",
    "        optimal_params.append({\n",
    "            'noise_level': noise,\n",
    "            'sigma': noise_data['sigma'].iloc[0],\n",
    "            'gs_k': best_gs_k,\n",
    "            'gs_mse': best_gs_mse,\n",
    "            'gs_df': best_gs_df,\n",
    "            'rgs_m': m_val,\n",
    "            'rgs_k': k_val,\n",
    "            'rgs_mse': best_rgs_mse,\n",
    "            'rgs_df': best_rgs_df,\n",
    "            **baseline_metrics\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum deviation from target in gram matrix: 4.88e-15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1249959ed978493890bfc07f9c2993f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Progress:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run simulation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m results_df, summary \u001b[38;5;241m=\u001b[39m \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_predictors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignal_proportion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.04\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbanded\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_sim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mrun_simulation\u001b[1;34m(n_predictors, n_train, signal_proportion, cov, n_sim)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k_max \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[0;32m    127\u001b[0m     rgs \u001b[38;5;241m=\u001b[39m FastRandomizedGreedySelection(k_max\u001b[38;5;241m=\u001b[39mk_max, m\u001b[38;5;241m=\u001b[39mm, n_resample_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m--> 128\u001b[0m     \u001b[43mrgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m     y_pred_rgs \u001b[38;5;241m=\u001b[39m rgs\u001b[38;5;241m.\u001b[39mpredict(X, k\u001b[38;5;241m=\u001b[39mk_max)\n\u001b[0;32m    130\u001b[0m     result[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse_rgs_m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_k\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk_max\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m mean_squared_error(y_true, y_pred_rgs)\n",
      "File \u001b[1;32m~\\Documents\\Princeton\\Research\\RFS\\notebooks\\..\\RGS.py:201\u001b[0m, in \u001b[0;36mFastRandomizedGreedySelection.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    199\u001b[0m mask[M] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    200\u001b[0m M_comp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp)[mask]\n\u001b[1;32m--> 201\u001b[0m correlations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[43mX_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM_comp\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m residuals)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# Generate new feature subsets\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_sets[k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_new_feature_sets(M, M_comp, correlations, freqs[i], generator)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run simulation\n",
    "results_df, summary = run_simulation(n_predictors=250, n_train=2000, signal_proportion=0.04, cov='banded', n_sim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal parameters for each noise level\n",
    "optimal_params = analyze_optimal_parameters(results_df)\n",
    "\n",
    "# View results\n",
    "print(\"Optimal parameters for each noise level:\")\n",
    "print(optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_mse_by_sigma\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m..\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msimulation_results_corr_exact_sparsity.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../figures/mse_by_sigma_corr_exact_sparsity.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Princeton\\Research\\RFS\\notebooks\\..\\data_plotting.py:34\u001b[0m, in \u001b[0;36mplot_mse_by_sigma\u001b[1;34m(csv_path, save_path)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# For each row, get optimal configurations\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[1;32m---> 34\u001b[0m     optimal_gs_mse\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_optimal_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpen_gs_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmse_gs_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     35\u001b[0m     optimal_rgs_mse\u001b[38;5;241m.\u001b[39mappend(get_optimal_configuration(df, idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpen_rgs_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse_rgs_\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Add optimal values to dataframe\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\Princeton\\Research\\RFS\\notebooks\\..\\data_plotting.py:16\u001b[0m, in \u001b[0;36mget_optimal_configuration\u001b[1;34m(df, row_idx, pen_prefix, mse_prefix)\u001b[0m\n\u001b[0;32m     13\u001b[0m penalties \u001b[38;5;241m=\u001b[39m {col: df\u001b[38;5;241m.\u001b[39miloc[row_idx][col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m pen_cols}\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Find configuration with minimum penalty\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m min_pen_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpenalties\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Get corresponding MSE column\u001b[39;00m\n\u001b[0;32m     19\u001b[0m config_suffix \u001b[38;5;241m=\u001b[39m min_pen_col\u001b[38;5;241m.\u001b[39mreplace(pen_prefix, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "plot_mse_by_sigma(\"..\\data\\simulation_results_corr_exact_sparsity.csv\", save_path=\"../figures/mse_by_sigma_corr_exact_sparsity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_df_vs_k(\"..\\data\\simulation_results_corr_exact_sparsity.csv\", 30, save_path=\"../figures/df_vs_k_sigma10_corr_exact_sparsity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
