{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/cyu/Documents/Princeton/Research/RFS/rgs\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting numpy>=1.20.0 (from RGS==0.1.0)\n",
      "  Using cached numpy-2.2.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting scipy>=1.7.0 (from RGS==0.1.0)\n",
      "  Using cached scipy-1.15.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Using cached numpy-2.2.2-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached scipy-1.15.1-cp312-cp312-win_amd64.whl (43.6 MB)\n",
      "Building wheels for collected packages: RGS\n",
      "  Building editable for RGS (pyproject.toml): started\n",
      "  Building editable for RGS (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for RGS: filename=RGS-0.1.0-0.editable-py3-none-any.whl size=1234 sha256=664e0f01113a996fb7ce367fc051ecd885245e3797e043dd975b212431c82890\n",
      "  Stored in directory: C:\\Users\\cyu\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ttpxwgpa\\wheels\\8a\\a0\\1e\\5f5b4524926ea595f44d868c60ef2e1d0a855310076ea2b30b\n",
      "Successfully built RGS\n",
      "Installing collected packages: numpy, scipy, RGS\n",
      "Successfully installed RGS-0.1.0 numpy-2.2.2 scipy-1.15.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.2 which is incompatible.\n",
      "numba 0.59.1 requires numpy<1.27,>=1.22, but you have numpy 2.2.2 which is incompatible.\n",
      "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.2.2 which is incompatible.\n",
      "streamlit 1.32.0 requires numpy<2,>=1.19.3, but you have numpy 2.2.2 which is incompatible.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\numpy already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\numpy-2.2.2-cp312-cp312-win_amd64.whl already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\numpy-2.2.2.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\numpy.libs already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\RGS-0.1.0.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\scipy already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\scipy-1.15.1-cp312-cp312-win_amd64.whl already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\scipy-1.15.1.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\scipy.libs already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\__editable__.RGS-0.1.0.pth already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\bin already exists. Specify --upgrade to force replacement.\n"
     ]
    }
   ],
   "source": [
    "!pip install --target=\"C:\\Users\\cyu\\custom_packages\" -e C:\\Users\\cyu\\Documents\\Princeton\\Research\\RFS\\rgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/cyu/Documents/Princeton/Research/RFS/rgs_experiments\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting rgs (from rgs_experiments==0.1.0)\n",
      "  Using cached rgs-0.1.11-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting numpy>=1.20.0 (from rgs_experiments==0.1.0)\n",
      "  Using cached numpy-2.2.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting scipy>=1.7.0 (from rgs_experiments==0.1.0)\n",
      "  Using cached scipy-1.15.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting matplotlib>=3.4.0 (from rgs_experiments==0.1.0)\n",
      "  Using cached matplotlib-3.10.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting pandas>=1.3.0 (from rgs_experiments==0.1.0)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting seaborn>=0.11.0 (from rgs_experiments==0.1.0)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.4.0->rgs_experiments==0.1.0)\n",
      "  Using cached contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.4.0->rgs_experiments==0.1.0)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.4.0->rgs_experiments==0.1.0)\n",
      "  Using cached fonttools-4.56.0-cp312-cp312-win_amd64.whl.metadata (103 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.4.0->rgs_experiments==0.1.0)\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting packaging>=20.0 (from matplotlib>=3.4.0->rgs_experiments==0.1.0)\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib>=3.4.0->rgs_experiments==0.1.0)\n",
      "  Using cached pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.4.0->rgs_experiments==0.1.0)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting python-dateutil>=2.7 (from matplotlib>=3.4.0->rgs_experiments==0.1.0)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.3.0->rgs_experiments==0.1.0)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.3.0->rgs_experiments==0.1.0)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting itu.algs4 (from rgs->rgs_experiments==0.1.0)\n",
      "  Using cached itu.algs4-0.2.5-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting numdifftools (from rgs->rgs_experiments==0.1.0)\n",
      "  Using cached numdifftools-0.9.41-py2.py3-none-any.whl.metadata (39 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib>=3.4.0->rgs_experiments==0.1.0)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached matplotlib-3.10.0-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "Using cached numpy-2.2.2-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached scipy-1.15.1-cp312-cp312-win_amd64.whl (43.6 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached rgs-0.1.11-py3-none-any.whl (62 kB)\n",
      "Using cached contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.56.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Using cached pillow-11.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Using cached itu.algs4-0.2.5-py3-none-any.whl (160 kB)\n",
      "Using cached numdifftools-0.9.41-py2.py3-none-any.whl (100 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: rgs_experiments\n",
      "  Building editable for rgs_experiments (pyproject.toml): started\n",
      "  Building editable for rgs_experiments (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for rgs_experiments: filename=rgs_experiments-0.1.0-0.editable-py3-none-any.whl size=1413 sha256=9d4e6a13b74f9cedce6a4b9e0bb4c8c0bf83c3c72f5ac88a0feba0681d33f197\n",
      "  Stored in directory: C:\\Users\\cyu\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-j83a7dq5\\wheels\\9b\\ee\\45\\9af17ba22777d1a6653baf21259f317af94c47b8ed3b210612\n",
      "Successfully built rgs_experiments\n",
      "Installing collected packages: pytz, itu.algs4, tzdata, six, pyparsing, pillow, packaging, numpy, kiwisolver, fonttools, cycler, scipy, python-dateutil, contourpy, pandas, numdifftools, matplotlib, seaborn, rgs, rgs_experiments\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 itu.algs4-0.2.5 kiwisolver-1.4.8 matplotlib-3.10.0 numdifftools-0.9.41 numpy-2.2.2 packaging-24.2 pandas-2.2.3 pillow-11.1.0 pyparsing-3.2.1 python-dateutil-2.9.0.post0 pytz-2025.1 rgs-0.1.11 rgs_experiments-0.1.0 scipy-1.15.1 seaborn-0.13.2 six-1.17.0 tzdata-2025.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.59.1 requires numpy<1.27,>=1.22, but you have numpy 2.2.2 which is incompatible.\n",
      "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.2.2 which is incompatible.\n",
      "streamlit 1.32.0 requires numpy<2,>=1.19.3, but you have numpy 2.2.2 which is incompatible.\n",
      "streamlit 1.32.0 requires packaging<24,>=16.8, but you have packaging 24.2 which is incompatible.\n",
      "streamlit 1.32.0 requires pillow<11,>=7.1.0, but you have pillow 11.1.0 which is incompatible.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\contourpy already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\contourpy-1.3.1.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\cycler already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\cycler-0.12.1.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\dateutil already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\fontTools already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\fonttools-4.56.0.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\itu already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\itu.algs4-0.2.5.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\kiwisolver already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\kiwisolver-1.4.8.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\matplotlib already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\matplotlib-3.10.0.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\mpl_toolkits already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\numdifftools already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\numdifftools-0.9.41.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\numpy already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\numpy-2.2.2-cp312-cp312-win_amd64.whl already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\numpy-2.2.2.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\numpy.libs already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\packaging already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\packaging-24.2.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\pandas already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\pandas-2.2.3.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\pandas.libs already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\PIL already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\pillow-11.1.0.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\pylab.py already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\pyparsing already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\pyparsing-3.2.1.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\python_dateutil-2.9.0.post0.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\pytz already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\pytz-2025.1.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\rgs already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\rgs-0.1.11.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\rgs_experiments-0.1.0.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\scipy already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\scipy-1.15.1-cp312-cp312-win_amd64.whl already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\scipy-1.15.1.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\scipy.libs already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\seaborn already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\seaborn-0.13.2.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\six-1.17.0.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\six.py already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\tzdata already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\tzdata-2025.1.dist-info already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\__editable__.rgs_experiments-0.1.0.pth already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\__pycache__ already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\bin already exists. Specify --upgrade to force replacement.\n",
      "WARNING: Target directory C:\\Users\\cyu\\custom_packages\\share already exists. Specify --upgrade to force replacement.\n"
     ]
    }
   ],
   "source": [
    "!pip install --target=\"C:\\Users\\cyu\\custom_packages\" -e C:\\Users\\cyu\\Documents\\Princeton\\Research\\RFS\\rgs_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from typing import List\n",
    "import time\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from time import sleep\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\cyu\\custom_packages\")\n",
    "from rgs import *\n",
    "from rgs_experiments import *\n",
    "# from data_plotting import *\n",
    "# from data_plotting_test import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgs_experiments.utils.sim_util_dgs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_df(y, y_true, y_pred, n, sigma):\n",
    "    \"\"\"Calculate degrees of freedom using the provided formula.\"\"\"\n",
    "    train_error = np.mean((y - y_pred)**2)\n",
    "    insample_error = np.mean((y_true - y_pred)**2)\n",
    "    df = n/(2*sigma**2) * (insample_error - train_error + sigma**2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_penalized_score(model, X, y_true, k, sigma2, n, p):\n",
    "    \"\"\"\n",
    "    Compute penalized score using MSE + penalty.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : RGS/GS model\n",
    "        Fitted model with .coef_ attribute\n",
    "    X : ndarray\n",
    "        Input design matrix\n",
    "    y_true : ndarray\n",
    "        True signal (without noise)\n",
    "    k : int\n",
    "        Number of features to use\n",
    "    sigma2 : float\n",
    "        True noise variance\n",
    "    n : int\n",
    "        Sample size\n",
    "    p : int\n",
    "        Number of features\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Penalized score (MSE + penalty)\n",
    "    \"\"\"\n",
    "    # Get predictions and MSE\n",
    "    y_pred = model.predict(X, k=k)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    # Compute penalty\n",
    "    penalty = 2*sigma2**2/n*k*np.log(p/k)\n",
    "    final_score = -(mse+penalty)\n",
    "    # print(f\"Inside scorer - MSE: {-mse}, Penalty: {-penalty}, Final: {final_score}\")\n",
    "    \n",
    "    return mse + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressDisplay:\n",
    "    def __init__(self, total, desc=\"Progress\"):\n",
    "        self.total = total\n",
    "        self.desc = desc\n",
    "        self.current = 0\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def update(self, n=1):\n",
    "        self.current += n\n",
    "        self._display_progress()\n",
    "        \n",
    "    def _display_progress(self):\n",
    "        clear_output(wait=True)\n",
    "        percentage = (self.current / self.total) * 100\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        rate = self.current / elapsed_time if elapsed_time > 0 else 0\n",
    "        eta = (self.total - self.current) / rate if rate > 0 else 0\n",
    "        \n",
    "        progress_bar = f\"[{'=' * int(percentage/2)}{' ' * (50-int(percentage/2))}]\"\n",
    "        print(f\"{self.desc}: {progress_bar} {percentage:.1f}%\")\n",
    "        print(f\"Progress: {self.current}/{self.total}\")\n",
    "        print(f\"Elapsed: {elapsed_time:.1f}s, ETA: {eta:.1f}s, Rate: {rate:.1f} it/s\")\n",
    "\n",
    "def run_simulation(n_predictors=500, n_train=2000, signal_proportion=0.02, \n",
    "                           cov='banded', n_sim=10, seed=42):\n",
    "    \"\"\"Run simulation finding optimal (m,k) pairs without recomputing.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate base design matrix\n",
    "    X_generators = {\n",
    "        'orthogonal': generate_orthogonal_X,\n",
    "        'banded': generate_banded_X\n",
    "    }\n",
    "    X_generator = X_generators[cov] if isinstance(cov, str) else cov\n",
    "    X = X_generator(n_predictors, n_train, seed=seed)\n",
    "    \n",
    "    # Define k values for training and m values for RGS\n",
    "    k_values = [i for i in range(1, 17)] \n",
    "    base = 2\n",
    "    num_points = 20\n",
    "    m_values = [int(2 + (n_predictors-2) * (base**x - 1)/(base**(num_points-1) - 1)) \n",
    "                for x in range(num_points)]\n",
    "    k_max = 35  # Maximum k value we're interested in\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "    \n",
    "    # Define example generators with different noise levels\n",
    "    sigma_dict = {\n",
    "        'orthogonal': [1],\n",
    "        'banded': [1, 3, 5, 10, 15]\n",
    "    }\n",
    "    sigmas = sigma_dict[cov]\n",
    "    example_generators = {f'sigma_{sigma}': generate_exact_sparsity_example \n",
    "                        for sigma in sigmas}\n",
    "    \n",
    "    # Initialize progress display\n",
    "    total_iterations = n_sim * len(example_generators)\n",
    "    progress = ProgressDisplay(total_iterations, desc=\"Simulation Progress\")\n",
    "    \n",
    "    for sim in range(1, n_sim+1):\n",
    "        for i, (noise_level, generator) in enumerate(example_generators.items()):\n",
    "            # Generate data\n",
    "            X, y, y_true, beta_true, p, sigma = generator(X, signal_proportion, \n",
    "                                                        sigmas[i], seed=seed+sim)\n",
    "            \n",
    "            # Fit baseline models\n",
    "            # Lasso\n",
    "            lasso = LassoCV(cv=10, random_state=seed+sim)\n",
    "            lasso.fit(X, y)\n",
    "            y_pred_lasso = lasso.predict(X)\n",
    "            \n",
    "            # Ridge\n",
    "            ridge = RidgeCV(cv=10)\n",
    "            ridge.fit(X, y)\n",
    "            y_pred_ridge = ridge.predict(X)\n",
    "            \n",
    "            # Elastic Net\n",
    "            elastic = ElasticNetCV(cv=10, random_state=seed+sim)\n",
    "            elastic.fit(X, y)\n",
    "            y_pred_elastic = elastic.predict(X)\n",
    "            \n",
    "            # Store base results with all metrics\n",
    "            base_result = {\n",
    "                'simulation': sim,\n",
    "                'noise_level': noise_level,\n",
    "                'sigma': sigma,\n",
    "                # Lasso metrics\n",
    "                'mse_lasso': mean_squared_error(y_true, y_pred_lasso),\n",
    "                'df_lasso': calculate_df(y, y_true, y_pred_lasso, n_train, sigma),\n",
    "                'coef_recovery_lasso': np.mean((lasso.coef_ - beta_true)**2),\n",
    "                'support_recovery_lasso': np.mean((lasso.coef_ != 0) == (beta_true != 0)),\n",
    "                # Ridge metrics\n",
    "                'mse_ridge': mean_squared_error(y_true, y_pred_ridge),\n",
    "                'df_ridge': calculate_df(y, y_true, y_pred_ridge, n_train, sigma),\n",
    "                'coef_recovery_ridge': np.mean((ridge.coef_ - beta_true)**2),\n",
    "                # Elastic Net metrics\n",
    "                'mse_elastic': mean_squared_error(y_true, y_pred_elastic),\n",
    "                'df_elastic': calculate_df(y, y_true, y_pred_elastic, n_train, sigma),\n",
    "                'coef_recovery_elastic': np.mean((elastic.coef_ - beta_true)**2),\n",
    "                'support_recovery_elastic': np.mean((elastic.coef_ != 0) == (beta_true != 0))\n",
    "            }\n",
    "            \n",
    "            # Fit GS for each k value\n",
    "            gs = RGS(k_max=k_max, m=n_predictors, n_resample_iter=7, random_state=seed+sim)\n",
    "            gs.fit(X, y)\n",
    "            for k in k_values:\n",
    "                y_pred_gs = gs.predict(X, k=k)\n",
    "                base_result.update({\n",
    "                    f'mse_gs_k{k}': mean_squared_error(y_true, y_pred_gs),\n",
    "                    f'df_gs_k{k}': calculate_df(y, y_true, y_pred_gs, n_train, sigma),\n",
    "                    f'pen_gs_k{k}': compute_penalized_score(gs, X, y, k, sigma, n_train, p),\n",
    "                    f'coef_recovery_gs_k{k}': np.mean((gs.coef_[k] - beta_true)**2),\n",
    "                    f'support_recovery_gs_k{k}': np.mean((np.abs(gs.coef_[k]) > 1e-10) == (beta_true != 0))\n",
    "                })\n",
    "            \n",
    "            # Fit RGS for each m,k value\n",
    "            for m in m_values:\n",
    "                # Train RGS once with maximum k\n",
    "                rgs = RGS(k_max=k_max, m=m, n_resample_iter=7, random_state=seed+sim)\n",
    "                rgs.fit(X, y)\n",
    "                \n",
    "                for k in k_values:\n",
    "                    y_pred = rgs.predict(X, k=k)\n",
    "                    # print(f\"y_pred mean/std: {np.mean(y_pred):.3f}/{np.std(y_pred):.3f}\")\n",
    "                    \n",
    "                    # Create new result with all baseline metrics\n",
    "                    result = base_result.copy()\n",
    "                    result.update({\n",
    "                        'm': m,\n",
    "                        'k': k,\n",
    "                        f'mse_rgs_m{m}_k{k}': mean_squared_error(y_true, y_pred),\n",
    "                        f'df_rgs_m{m}_k{k}': calculate_df(y, y_true, y_pred, n_train, sigma),\n",
    "                        f'pen_rgs_m{m}_k{k}': compute_penalized_score(rgs, X, y, k, sigma, n_train, p),\n",
    "                        f'coef_recovery_rgs_m{m}_k{k}': np.mean((rgs.coef_[k] - beta_true)**2),\n",
    "                        f'support_recovery_rgs_m{m}_k{k}': np.mean((np.abs(rgs.coef_[k]) > 1e-10) == (beta_true != 0))\n",
    "                    })\n",
    "                    results.append(result)\n",
    "                \n",
    "            # Update progress\n",
    "            progress.update(1)\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    summary_metrics = {\n",
    "        # Baseline metrics\n",
    "        'mse_lasso': ['mean', 'std'],\n",
    "        'df_lasso': ['mean', 'std'],\n",
    "        'coef_recovery_lasso': ['mean', 'std'],\n",
    "        'support_recovery_lasso': ['mean', 'std'],\n",
    "        'mse_ridge': ['mean', 'std'],\n",
    "        'df_ridge': ['mean', 'std'],\n",
    "        'coef_recovery_ridge': ['mean', 'std'],\n",
    "        'mse_elastic': ['mean', 'std'],\n",
    "        'df_elastic': ['mean', 'std'],\n",
    "        'coef_recovery_elastic': ['mean', 'std'],\n",
    "        'support_recovery_elastic': ['mean', 'std']\n",
    "    }\n",
    "    \n",
    "    # Add GS metrics for each k\n",
    "    for k in k_values:\n",
    "        metrics = ['mse', 'df', 'pen', 'coef_recovery', 'support_recovery']\n",
    "        for metric in metrics:\n",
    "            summary_metrics[f'{metric}_gs_k{k}'] = ['mean', 'std']\n",
    "    \n",
    "    # Add RGS metrics for each (m,k) combination\n",
    "    for m in m_values:\n",
    "        for k in k_values:\n",
    "            metrics = ['mse', 'df', 'pen', 'coef_recovery', 'support_recovery']\n",
    "            for metric in metrics:\n",
    "                summary_metrics[f'{metric}_rgs_m{m}_k{k}'] = ['mean', 'std']\n",
    "    \n",
    "    summary = results_df.groupby('noise_level').agg(summary_metrics).round(4)\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_df.to_csv(f'../simulation_results_{timestamp}.csv', index=False)\n",
    "    summary.to_csv(f'../simulation_summary_{timestamp}.csv')\n",
    "    \n",
    "    print(f\"\\nSimulation completed in {(time.time() - start_time)/60:.1f} minutes\")\n",
    "    \n",
    "    return results_df, summary\n",
    "\n",
    "def plot_optimal_mk(optimal_params_df):\n",
    "    \"\"\"Plot optimal k values for each m and noise level.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for noise_level in optimal_params_df['noise_level'].unique():\n",
    "        data = optimal_params_df[optimal_params_df['noise_level'] == noise_level]\n",
    "        mean_k = data.groupby('m')['optimal_k'].mean()\n",
    "        std_k = data.groupby('m')['optimal_k'].std()\n",
    "        \n",
    "        plt.errorbar(mean_k.index, mean_k.values, yerr=std_k.values, \n",
    "                    label=noise_level, marker='o')\n",
    "    \n",
    "    plt.xlabel('m (number of candidates)')\n",
    "    plt.ylabel('Optimal k')\n",
    "    plt.title('Optimal k vs m for Different Noise Levels')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_optimal_parameters(results_df):\n",
    "    \"\"\"\n",
    "    Analyze the results to find optimal k and m values for each noise level.\n",
    "    Also includes degrees of freedom analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : DataFrame\n",
    "        Results from run_simulation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with optimal parameters and corresponding MSE and DF values\n",
    "    \"\"\"\n",
    "    noise_levels = sorted(results_df['noise_level'].unique())\n",
    "    optimal_params = []\n",
    "    \n",
    "    for noise in noise_levels:\n",
    "        noise_data = results_df[results_df['noise_level'] == noise]\n",
    "        \n",
    "        # Find best GS k value based on penalized\n",
    "        gs_mse_cols = [col for col in noise_data.columns if col.startswith('pen_gs_k')]\n",
    "        best_gs_col = min(gs_mse_cols, key=lambda x: noise_data[x].mean())\n",
    "        best_gs_k = int(best_gs_col.replace('pen_gs_k', ''))\n",
    "        best_gs_mse = noise_data[f'mse_gs_k{best_gs_k}'].mean()\n",
    "        best_gs_df = noise_data[f'df_gs_k{best_gs_k}'].mean()\n",
    "        \n",
    "        # Find best RGS m and k values based on penalized\n",
    "        rgs_mse_cols = [col for col in noise_data.columns if col.startswith('pen_rgs_m')]\n",
    "        best_rgs_col = min(rgs_mse_cols, key=lambda x: noise_data[x].mean())\n",
    "        # Extract m and k values using string replacement\n",
    "        col_parts = best_rgs_col.replace('pen_rgs_m', '').split('_k')\n",
    "        m_val = int(col_parts[0])\n",
    "        k_val = int(col_parts[1])\n",
    "        best_rgs_mse = noise_data[f'mse_rgs_m{m_val}_k{k_val}'].mean()\n",
    "        best_rgs_df = noise_data[f'df_rgs_m{m_val}_k{k_val}'].mean()\n",
    "        \n",
    "        # Get baseline methods performance\n",
    "        baseline_metrics = {\n",
    "            'lasso_mse': noise_data['mse_lasso'].mean(),\n",
    "            'lasso_df': noise_data['df_lasso'].mean(),\n",
    "            'ridge_mse': noise_data['mse_ridge'].mean(),\n",
    "            'ridge_df': noise_data['df_ridge'].mean(),\n",
    "            'elastic_mse': noise_data['mse_elastic'].mean(),\n",
    "            'elastic_df': noise_data['df_elastic'].mean()\n",
    "        }\n",
    "        \n",
    "        # Store results\n",
    "        optimal_params.append({\n",
    "            'noise_level': noise,\n",
    "            'sigma': noise_data['sigma'].iloc[0],\n",
    "            'gs_k': best_gs_k,\n",
    "            'gs_mse': best_gs_mse,\n",
    "            'gs_df': best_gs_df,\n",
    "            'rgs_m': m_val,\n",
    "            'rgs_k': k_val,\n",
    "            'rgs_mse': best_rgs_mse,\n",
    "            'rgs_df': best_rgs_df,\n",
    "            **baseline_metrics\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation Progress: [==================================================] 100.0%\n",
      "Progress: 5/5\n",
      "Elapsed: 229.6s, ETA: 0.0s, Rate: 0.0 it/s\n",
      "\n",
      "Simulation completed in 3.8 minutes\n"
     ]
    }
   ],
   "source": [
    "# Run simulation\n",
    "results_df, summary = run_simulation(n_predictors=50, n_train=1000, signal_proportion=0.2, \n",
    "                           cov='orthogonal', n_sim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters for each noise level:\n",
      "  noise_level  sigma  gs_k    gs_mse      gs_df  rgs_m  rgs_k  rgs_mse  \\\n",
      "0     sigma_1      1    16  0.030072  31.821705     25     16  0.02867   \n",
      "\n",
      "      rgs_df  lasso_mse   lasso_df  ridge_mse   ridge_df  elastic_mse  \\\n",
      "0  31.671734   0.029979  26.979588   0.048298  49.440896     0.037394   \n",
      "\n",
      "   elastic_df  \n",
      "0   36.682955  \n"
     ]
    }
   ],
   "source": [
    "# Find optimal parameters for each noise level\n",
    "optimal_params = analyze_optimal_parameters(results_df)\n",
    "\n",
    "# View results\n",
    "print(\"Optimal parameters for each noise level:\")\n",
    "print(optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_mse_by_sigma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_mse_by_sigma(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/simulation_results_ortho_nonlinear.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../figures/mse_by_sigma_ortho_nonlinear.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_mse_by_sigma' is not defined"
     ]
    }
   ],
   "source": [
    "plot_mse_by_sigma(\"../data/simulation_results_ortho_nonlinear.csv\", save_path=\"../figures/mse_by_sigma_ortho_nonlinear.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_df_vs_k(\"../data/simulation_results_ortho_exact_final.csv\", 40, save_path=\"../figures/df_vs_k_sigma40_ortho_exact_final.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse_by_variance_explained(\"../data/simulation_results_ortho_nonlinear_eta10.csv\", save_path=\"../figures/mse_by_variance_explained_ortho_nonlinear_eta10.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def create_penalized_scorer(sigma, n, p):\n",
    "    \"\"\"\n",
    "    Create a base scorer function that can be wrapped for each k value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma2 : float\n",
    "        True noise variance\n",
    "    n : int\n",
    "        Sample size\n",
    "    p : int\n",
    "        Number of features\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    callable\n",
    "        Function that creates sklearn-compatible scorer for a given k\n",
    "    \"\"\"\n",
    "    def make_k_scorer(k):\n",
    "        \"\"\"Create a scorer for a specific k value.\"\"\"\n",
    "        def penalized_score(y, y_pred):\n",
    "            # Compute MSE\n",
    "            error = y - y_pred\n",
    "            mse = (error ** 2).mean()\n",
    "            \n",
    "            # Compute penalty using current k value\n",
    "            penalty = 2*sigma**2/n*k*np.log(p/k)\n",
    "            \n",
    "            # Return negative since sklearn maximizes scores\n",
    "            return -(mse + penalty)\n",
    "        \n",
    "        # Create sklearn-compatible scorer\n",
    "        return make_scorer(penalized_score)\n",
    "    \n",
    "    return make_k_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "Test passed! Scoring functions produce equivalent results.\n",
      "sklearn_score: 0.2512755213891126\n",
      "direct_score: 0.2512755213891126\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pytest\n",
    "\n",
    "# Mock model class that mimics the interface needed\n",
    "class MockRGSModel:\n",
    "    def __init__(self, coef):\n",
    "        self.coef_ = coef\n",
    "        \n",
    "    def predict(self, X, k=None):\n",
    "        if k is None:\n",
    "            k = len(self.coef_)\n",
    "        # Use only top k coefficients\n",
    "        temp_coef = np.zeros_like(self.coef_)\n",
    "        top_k_idx = np.argsort(np.abs(self.coef_))[-k:]\n",
    "        temp_coef[top_k_idx] = self.coef_[top_k_idx]\n",
    "        return X @ temp_coef\n",
    "\n",
    "def test_scoring_functions_equivalence():\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate test data\n",
    "    n, p = 100, 20\n",
    "    X = np.random.randn(n, p)\n",
    "    print(X.shape[1])\n",
    "    true_beta = np.zeros(p)\n",
    "    true_beta[:5] = [1, 0.8, 0.6, 0.4, 0.2]  # Only 5 true non-zero coefficients\n",
    "    \n",
    "    # Generate response with some noise\n",
    "    sigma2 = 0.1\n",
    "    noise = np.random.normal(0, np.sqrt(sigma2), n)\n",
    "    y = X @ true_beta + noise\n",
    "    \n",
    "    # Create mock model with some coefficients\n",
    "    model = MockRGSModel(true_beta + np.random.normal(0, 0.1, p))\n",
    "    \n",
    "    # Test parameters\n",
    "    k = 5  # Number of features to use\n",
    "    \n",
    "    # Create scorer using the function from penalized_score.py\n",
    "    make_k_scorer = create_penalized_scorer(sigma2, n, p)\n",
    "    sklearn_scorer = make_k_scorer(k)\n",
    "    \n",
    "    # Compute score using the sklearn scorer\n",
    "    # Note: sklearn scorer returns negative of the score\n",
    "    sklearn_score = -sklearn_scorer._score_func(y, model.predict(X, k=k))\n",
    "    \n",
    "    # Compute score using the direct function\n",
    "    direct_score = compute_penalized_score(model, X, y, k, sigma2, n, p)\n",
    "    \n",
    "    # Test that scores are approximately equal\n",
    "    np.testing.assert_almost_equal(\n",
    "        sklearn_score,\n",
    "        direct_score,\n",
    "        decimal=5,\n",
    "        err_msg=\"Scoring functions produced different results\"\n",
    "    )\n",
    "    \n",
    "    print(\"Test passed! Scoring functions produce equivalent results.\")\n",
    "    print(f\"sklearn_score: {sklearn_score}\")\n",
    "    print(f\"direct_score: {direct_score}\")\n",
    "\n",
    "# Run the test\n",
    "if __name__ == \"__main__\":\n",
    "    test_scoring_functions_equivalence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
